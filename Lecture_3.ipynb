{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPC9u7sitrIS+vIgkvIuq+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SandytheTraveller/Deep-Learning-Course/blob/main/Lecture_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 3"
      ],
      "metadata": {
        "id": "51p3_MatZ_8a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUbUnbQYZWa1"
      },
      "outputs": [],
      "source": [
        "def plot_decision_boundary(X, y, w, title):\n",
        "    # w - current weights\n",
        "    # X - Input vectors\n",
        "    # y - input labels\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.rc('axes', labelsize=14) # fontsize of the x and y labels\n",
        "    plt.rc('xtick', labelsize=14) # fontsize of the tick labels\n",
        "    plt.rc('ytick', labelsize=14)\n",
        "    w = model.w\n",
        "    m = -w[0]/w[1] # slope\n",
        "    print(f'w:{w}, m:{m}')\n",
        "\n",
        "    last_x = X[len(y) - 1, :]\n",
        "    print(f'last_x:{last_x}')\n",
        "\n",
        "    plt.suptitle(title+'_i_'+str(len(X)), fontsize=20)\n",
        "    plt.ylim(-3, 3)\n",
        "    plt.xlim(-3, 3)\n",
        "\n",
        "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bo\", label=\"Neg\", markersize=15)\n",
        "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"ro\", label=\"Pos\", markersize=15)\n",
        "    xx = np.linspace(-3,3)\n",
        "    plt.xlabel(\"X0\")\n",
        "    plt.ylabel(\"X1\")\n",
        "    #drawing hyperplane\n",
        "    plt.plot(xx, m*xx, 'r-') # hyperplane\n",
        "\n",
        "    plt.legend(loc=\"upper left\",prop={'size': 15})\n",
        "    plt.grid()\n",
        "    # scaling to have norm 2\n",
        "    #scale=np.sqrt(1/(w[0]**2+w[1]**2))\n",
        "\n",
        "    # drawing vector throug origin and w\n",
        "    #plt.arrow(0,0, scale*w[0],scale*w[1], linestyle=\"--\",\n",
        "    plt.arrow(0,0, w[0], w[1], linestyle=\"--\",\n",
        "          head_width = 0.09,\n",
        "          width = 0.02,\n",
        "          ec ='red')\n",
        "    # drawing vector through origin and current sample\n",
        "    plt.arrow(0,0, last_x[0],last_x[1], linestyle=\"--\",\n",
        "          head_width = 0.09,\n",
        "          width = 0.02,\n",
        "          ec ='green')\n",
        "    #plt.pause(5)\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To visuzlize the hyperplane during the training, we write the fit_print method that during training visualizes the hyperplane after each sample update\n",
        "- Unlike the fit method, this is a sequential update\n",
        "  - Within each epoch any weight update is right away used for the prediction of the next sample\n",
        "  - To better visualize the change, no bias is used during the training"
      ],
      "metadata": {
        "id": "U6YncbGJbmnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, eta):\n",
        "        self.w = None\n",
        "        #self.b = None\n",
        "        self.b = 0\n",
        "        self.eta = eta\n",
        "\n",
        "    # heaviside (step) activation function\n",
        "    def activation(self, z):\n",
        "        return np.heaviside(z, 0) # heaviside(z), 0 is the value for z=0\n",
        "\n",
        "    #  training Perceptron\n",
        "    def fit(self, X, y, epochs, performance_criterion):\n",
        "        n_features = X.shape[1]\n",
        "        # Initializing weights and bias\n",
        "        #self.w = np.random.randn(n_features)/np.sqrt(n_features)\n",
        "        self.w = np.zeros((n_features))\n",
        "        self.b = 0\n",
        "\n",
        "        # Iterating until the number of epochs\n",
        "        for epoch in range(epochs):\n",
        "          # self.w - current hyperplane\n",
        "          # we are predicting ALL our training samples before any update; parallel update\n",
        "          # any instance is predicted regardless of the updates of other instances\n",
        "          # sequential update - simulate online training\n",
        "            z = np.dot(X, self.w) + self.b # Computing the dot product and adding the bias\n",
        "            y_pred = self.activation(z) # Passing through an activation function\n",
        "            # Traversing through the entire training set\n",
        "            for i in range(len(X)):\n",
        "                #Updating weights and bias\n",
        "                self.w = self.w + self.eta * (y[i] - y_pred[i]) * X[i]\n",
        "                self.b = self.b + self.eta * (y[i] - y_pred[i])\n",
        "            print(f\"\\t epoch:{epoch}, accuracy:{performance_criterion(y, y_pred)}\")\n",
        "        return self.w, self.b\n",
        "\n",
        "    def fit_print(self, X, y, epochs):\n",
        "      # here we receive the instance, update the hyperplane, then receive the next one\n",
        "        n_features = X.shape[1]\n",
        "        n_samples = X.shape[0]\n",
        "        # Initializing weights and bias\n",
        "        #self.w = np.random.randn(n_features)/np.sqrt(n_features)\n",
        "        self.w = np.zeros((n_features)) + 0.1 # non zero init\n",
        "        self.b = 0\n",
        "\n",
        "        # Iterating until the number of epochs\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(n_samples):\n",
        "              if i > 0:\n",
        "                plot_decision_boundary(X[:(i+1), :],y[:(i+1)], self.w, \"initial\")\n",
        "\n",
        "              z = np.dot(X[i,:], self.w) + self.b\n",
        "              y_pred = self.activation(z)\n",
        "              #Updating weights and bias\n",
        "              self.w = self.w + self.eta * (y[i] - y_pred) * X[i]\n",
        "              #self.b = self.b + self.eta * (y[i] - y_pred[i])\n",
        "              if i > 0:\n",
        "                plot_decision_boundary(X[:(i+1), :],y[:(i+1)], self.w,\"update\")\n",
        "\n",
        "            # Traversing through the entire training set\n",
        "\n",
        "        return self.w, self.b\n",
        "\n",
        "    def predict(self, X):\n",
        "        z = np.dot(X, self.w) + self.b\n",
        "        return self.activation(z)"
      ],
      "metadata": {
        "id": "c2ezUyqrbnHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we classify the iris dataset"
      ],
      "metadata": {
        "id": "a0sc31MWd8bA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "iris.feature_names"
      ],
      "metadata": {
        "id": "euc1P9ufd_Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris.target_names"
      ],
      "metadata": {
        "id": "BmDQJbmHfRvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris.data.shape"
      ],
      "metadata": {
        "id": "nF-N-B4tfUXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris.target"
      ],
      "metadata": {
        "id": "VxxxdDAXfW5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X = iris.data # all columns\n",
        "X = iris.data[:, (0, 1)] # two features\n",
        "y = (iris.target == 0).astype(int) # we classify setosa against all\n",
        "y"
      ],
      "metadata": {
        "id": "wyaIU8N3fZSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we split the data in Train and Test set and normalize."
      ],
      "metadata": {
        "id": "GIxdklpAftOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True, stratify=y)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "8udRIHVafxoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the perceptron on the training data."
      ],
      "metadata": {
        "id": "z2S6iCJxgRjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## learning rate 1 (too high for a practical choice, here chosen just for the plot)\n",
        "model = Perceptron(1)\n",
        "print(f'model.w: {model.w}, model.b: {model.b}')\n",
        "# model.fit(X_train, y_train)\n",
        "model.fit_print(X_train, y_train, epochs=1)\n",
        "print(f'model.w: {model.w}, model.b: {model.b}')\n",
        "y_train_predicted = model.predict(X_train)\n",
        "y_test_predicted = model.predict(X_test)"
      ],
      "metadata": {
        "id": "ZKK1us6igUTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Better way to estimate the generalization error: cross-validation."
      ],
      "metadata": {
        "id": "3zRnWVSYh5m7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, StratifiedKFold\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "acc = []\n",
        "fold = 1\n",
        "\n",
        "for train_idx, test_idx in kf.split(X, y):\n",
        "    print(f\"fold:{fold}\")\n",
        "    X_train, y_train = X[train_idx,:], y[train_idx]\n",
        "    model = Perceptron(0.01)\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    model.fit(X_train, y_train, epochs=10,\n",
        "              performance_criterion=accuracy_score)\n",
        "    #print(f\"\\tmodel.w:{model.w}\")\n",
        "    X_test, y_test = X[test_idx,:], y[test_idx]\n",
        "    X_test = scaler.transform(X_test)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    print(f\"test accuracy fold:{accuracy}\\n\")\n",
        "    acc.append(accuracy)\n",
        "    fold+=1\n",
        "\n",
        "print(f\"mean accuracy:{np.mean(acc)}, sdt accuracy:{np.std(acc)}\")"
      ],
      "metadata": {
        "id": "-ZSO66Hlh-PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the perceptron boundary trained on last fold"
      ],
      "metadata": {
        "id": "UTsmaFtwoGMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_norm = scaler.fit_transform(X)\n",
        "plot_decision_boundary(X_norm,y, model.w,\"Final\")"
      ],
      "metadata": {
        "id": "wfzKgVemoF6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise**\n",
        "* ### By leveraging the perceptron class, implement a new class adaline, and modifiy the corresponding fit method to implement the adaline learning rule\n",
        "* ### Test it on iris and wine data"
      ],
      "metadata": {
        "id": "qGq4-tPqizuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))\n",
        "\n",
        "\n",
        "class Adaline:\n",
        "    def __init__(self, eta):\n",
        "        self.w = None\n",
        "        #self.b = None\n",
        "        self.b = 0\n",
        "        self.eta = eta\n",
        "\n",
        "    # heaviside (step) activation function\n",
        "    def activation(self, z):\n",
        "        return np.heaviside(z, 0) # heaviside(z), 0 is the value for z=0\n",
        "\n",
        "    #  training Perceptron\n",
        "    def fit(self, X, y, epochs, performance_criterion):\n",
        "        n_features = X.shape[1]\n",
        "        # Initializing weights and bias\n",
        "        #self.w = np.random.randn(n_features)/np.sqrt(n_features)\n",
        "        self.w = np.zeros((n_features))\n",
        "        self.b = 0\n",
        "\n",
        "        # Iterating until the number of epochs\n",
        "        for epoch in range(epochs):\n",
        "            z = np.dot(X, self.w) + self.b # Computing the dot product and adding the bias\n",
        "            y_pred = self.activation(z) # Passing through an activation function\n",
        "            # Traversing through the entire training set\n",
        "            for i in range(len(X)):\n",
        "                #Updating weights and bias\n",
        "                if y_pred[i] != y[i]: # if the prediction is wrong\n",
        "                    self.w = self.w + self.eta * (y[i] - z[i]) * X[i]\n",
        "                    self.b = self.b + self.eta * (y[i] - z[i])\n",
        "            print(f\"\\t epoch:{epoch}, accuracy:{performance_criterion(y, y_pred)}\")\n",
        "        return self.w, self.b\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        z = np.dot(X, self.w) + self.b\n",
        "        return self.activation(z)"
      ],
      "metadata": {
        "id": "cDvmGvLpit4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris, load_wine\n",
        "\n",
        "iris = load_iris()\n",
        "wine = load_wine()"
      ],
      "metadata": {
        "id": "b_WhozYNkbyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing on iris data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, shuffle=True)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "model = Perceptron(0.01)\n",
        "print(f'model.w: {model.w}, model.b: {model.b}')\n",
        "\n",
        "model.fit_print(X_train, y_train, epochs=1)\n",
        "print(f'model.w: {model.w}, model.b: {model.b}')\n",
        "\n",
        "y_train_predicted = model.predict(X_train)\n",
        "y_test_predicted = model.predict(X_test)"
      ],
      "metadata": {
        "id": "vGBOSpubkkXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wine.feature_names"
      ],
      "metadata": {
        "id": "J1hlatCQlFP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wine.target_names"
      ],
      "metadata": {
        "id": "8chvxt3Dlg2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wine.data.shape"
      ],
      "metadata": {
        "id": "rv1Bwz2RljmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wine.target"
      ],
      "metadata": {
        "id": "JH0HaVtSlw7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X = iris.data # all columns\n",
        "X = wine.data # two features\n",
        "y = (wine.target == 1).astype(int)\n",
        "y"
      ],
      "metadata": {
        "id": "pRYE9QbBlybN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y)"
      ],
      "metadata": {
        "id": "pX-F5r9bl8Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "model = Perceptron(0.01)\n",
        "print(f'model.w: {model.w}, model.b: {model.b}')\n",
        "\n",
        "model.fit_print(X_train, y_train, epochs=1)\n",
        "print(f'model.w: {model.w}, model.b: {model.b}')\n",
        "\n",
        "y_train_predicted = model.predict(X_train)\n",
        "y_test_predicted = model.predict(X_test)"
      ],
      "metadata": {
        "id": "8TUwM8v1mfxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalization capabilities of Adaline"
      ],
      "metadata": {
        "id": "NwdEAi6anTn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kf = StratifiedKFold(n_splits=5)\n",
        "acc = []\n",
        "fold=1\n",
        "\n",
        "for train_idx, test_idx in kf.split(X, y):\n",
        "    print(f\"fold:{fold}\")\n",
        "    X_train, y_train = X[train_idx,:], y[train_idx]\n",
        "    model_A = Adaline(0.001)\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    model_A.fit(X_train, y_train, epochs=10,\n",
        "              performance_criterion=accuracy_score)\n",
        "    #print(f\"\\tmodel.w:{model.w}\")\n",
        "    X_test, y_test = X[test_idx,:], y[test_idx]\n",
        "    X_test = scaler.transform(X_test)\n",
        "    y_test_pred = model_A.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    print(f\"test accuracy fold:{accuracy}\\n\")\n",
        "    acc.append(accuracy)\n",
        "    fold+=1\n",
        "\n",
        "print(f\"mean accuracy:{np.mean(acc)}, sdt accuracy:{np.std(acc)}\")"
      ],
      "metadata": {
        "id": "sMTVA6VdnPdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision boundary of ADALINE trained on last fold"
      ],
      "metadata": {
        "id": "-4JH7QjMnjLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_norm = scaler.fit_transform(X)\n",
        "plot_decision_boundary(X_norm, y, model_A.w,\"Final\")"
      ],
      "metadata": {
        "id": "WxvKL7Jqnj1K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}